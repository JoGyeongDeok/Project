{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoGyeongDeok/Project/blob/main/Dacon/2022_1_17_AI_competition_to_diagnose_crop_diseases_due_to_changes_in_the_agricultural_environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI competition to diagnose crop diseases due to changes in the agricultural environment\n",
        "***https://www.dacon.io/competitions/official/235870/overview/description***"
      ],
      "metadata": {
        "id": "RYrcxEeAT5eL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8znKRTOtklN_"
      },
      "source": [
        "# 1. Library & Data Load & 함수정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgsd5wE06GHT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuLMM1L_FN2G"
      },
      "outputs": [],
      "source": [
        "!pip install ray                                    #병렬처리 \n",
        "!git clone https://github.com/ultralytics/yolov5    #Yolo\n",
        "!cd yolov5; pip install -qr requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgKKGur649nE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5UJIazHya1r"
      },
      "outputs": [],
      "source": [
        "cp -r '/content/yolov5/utils' '/content'\n",
        "cp -r '/content/yolov5/models' '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ico0McsFs1Fy"
      },
      "outputs": [],
      "source": [
        "path= '/content/drive/MyDrive/DACON/Data/AI competition to diagnose crop diseases due to changes in the agricultural environment'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iRpcN8pzyFF"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import os\n",
        "import json \n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import yaml\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import ray\n",
        "import datetime\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-E_Gk2dFATH"
      },
      "outputs": [],
      "source": [
        "#os.mkdir(path)\n",
        "# # zipfile.ZipFile(path+'.zip','r').extractall(path+'/')\n",
        "# # zipfile.ZipFile(path+'/train.zip','r').extractall(path+'/')\n",
        "# # zipfile.ZipFile(path+'/test.zip','r').extractall(path+'/')\n",
        "\n",
        "#os.mkdir(path+'/images')\n",
        "#os.mkdir(path+'/labels')\n",
        "\n",
        "#os.mkdir(path+'/images/train')\n",
        "#os.mkdir(path+'/images/valid')\n",
        "#os.mkdir(path+'/images/test')\n",
        "\n",
        "#os.mkdir(path+'/labels/train')\n",
        "#os.mkdir(path+'/labels/valid')\n",
        "#os.mkdir(path+'/labels/test')\n",
        "\n",
        "# os.mkdir(path + '/ultra_workdir')\n",
        "# os.makedirs(path + '/CNN_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6LRSMlJlGq6"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print('Using PyTorch version : ', torch.__version__, 'Device : ', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZUpO4tladu_"
      },
      "source": [
        "# 2. Object Deteion 수행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMCbdX7rAy4"
      },
      "source": [
        "## Yolo 형식 맞춰서 이미지, annotation 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwGKzbDgWRGv"
      },
      "outputs": [],
      "source": [
        "test_index = []\n",
        "for _ in range(10):\n",
        "  if len(test_index) != 0 :\n",
        "    break\n",
        "\n",
        "  train_index = sorted(glob(path+\"/train/*\"))\n",
        "  test_index = sorted(glob(path+\"/test/*\"))\n",
        "  \n",
        "  for num in range(len(train_index)):\n",
        "    train_index[num] = train_index[num] + train_index[num][len(path + '/train'):]\n",
        "    \n",
        "  for num in range(len(test_index)):\n",
        "    test_index[num] = test_index[num] + test_index[num][len(path + '/test'):]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3q4tfwymmid",
        "outputId": "9516b43e-15bc-4b8a-cad5-822ee206116f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51906"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#51906\n",
        "len(test_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM7zmcG7HyPa",
        "outputId": "c86f477e-883d-48de-f739-c3e5f0874f27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5767"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#5767\n",
        "len(train_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLOTpxoXAq47"
      },
      "outputs": [],
      "source": [
        "# 1개의 voc xml 파일을 Yolo 포맷용 txt 파일로 변경하는 함수 \n",
        "def json_to_txt(input_xml_file, output_txt_file):\n",
        "  # 원본 이미지의 너비와 높이 추출. \n",
        "  with open(input_xml_file, 'r', encoding='UTF-8') as json_file:\n",
        "    json_file = json.load(json_file)\n",
        "    img_width = int(json_file['description']['width'])\n",
        "    img_height = int(json_file['description']['height'])\n",
        "    x1 = int(json_file['annotations']['bbox'][0]['x'])\n",
        "    y1 = int(json_file['annotations']['bbox'][0]['y'])\n",
        "    x2 = int(json_file['annotations']['bbox'][0]['x']) + int(json_file['annotations']['bbox'][0]['w'])\n",
        "    y2 = int(json_file['annotations']['bbox'][0]['y']) + int(json_file['annotations']['bbox'][0]['h'])\n",
        "    object_name = json_file['annotations']['crop']\n",
        "\n",
        "  # object_name과 원본 좌표를 입력하여 Yolo 포맷으로 변환하는 convert_yolo_coord()함수 호출. \n",
        "  cx_norm, cy_norm, w_norm, h_norm = convert_yolo_coord(object_name, img_width, img_height, x1, y1, x2, y2)\n",
        "  # 변환된 yolo 좌표를 object 별로 출력 text 파일에 write\n",
        "  value_str = ('{0} {1} {2} {3} {4}').format(object_name-1, cx_norm, cy_norm, w_norm, h_norm)\n",
        "  with open(output_txt_file, 'w') as output_fpointer:\n",
        "    output_fpointer.write(value_str+'\\n')\n",
        "\n",
        "\n",
        "# object_name과 원본 좌표를 입력하여 Yolo 포맷으로 변환\n",
        "def convert_yolo_coord(object_name, img_width, img_height, x1, y1, x2, y2):\n",
        "  # 중심 좌표와 너비, 높이 계산. \n",
        "  center_x = (x1 + x2)/2\n",
        "  center_y = (y1 + y2)/2\n",
        "  width = x2 - x1\n",
        "  height = y2 - y1\n",
        "  # 원본 이미지 기준으로 중심 좌표와 너비 높이를 0-1 사이 값으로 scaling\n",
        "  center_x_norm = center_x / img_width\n",
        "  center_y_norm = center_y / img_height\n",
        "  width_norm = width / img_width\n",
        "  height_norm = height / img_height\n",
        "\n",
        "  return round(center_x_norm, 7), round(center_y_norm, 7), round(width_norm, 7), round(height_norm, 7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.mov"
      ],
      "metadata": {
        "id": "k6k_BnC4gkbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS3-QTAQnzEJ"
      },
      "outputs": [],
      "source": [
        "def make_yolo_anno_file(index, tgt_images_dir, tgt_labels_dir, image, label):\n",
        "  for row in tqdm(index):\n",
        "    src_image_path = row + '.jpg'\n",
        "    src_label_path = row + '.json'\n",
        "    # yolo format으로 annotation할 txt 파일의 절대 경로명을 지정. \n",
        "      \n",
        "    if image == True : \n",
        "      # image의 경우 target images 디렉토리로 단순 copy\n",
        "      shutil.copy(src_image_path, tgt_images_dir)     \n",
        "\n",
        "    if label == True : \n",
        "      target_label_path = tgt_labels_dir + row[-5:]+'.txt'\n",
        "      # annotation의 경우 json 파일을 target labels 디렉토리에 Ultralytics Yolo format으로 변환하여  만듬\n",
        "      json_to_txt(src_label_path, target_label_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9W1e__hBZHL"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "valid_index = random.sample(train_index,int(len(train_index)*0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcezixSrmcFd"
      },
      "outputs": [],
      "source": [
        "# # # train용 images와 labels annotation 생성. \n",
        "# make_yolo_anno_file(train_index, path+'/images/train/', path+'/labels/train/', image = True, label = True)\n",
        "make_yolo_anno_file(valid_index, path+'/images/valid/', path+'/labels/valid/', image = True, label = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38FZ0NNyFpFL"
      },
      "outputs": [],
      "source": [
        "# yaml파일 작성\n",
        "#with open(path+\"/crop.yaml\",\"w\") as f:\n",
        "#  f.write(f'train: {path}/images/train/\\n')\n",
        "#  f.write(f'val: {path}/images/valid/\\n')  \n",
        "#  f.write(f'test: {path}/images/test/\\n')\n",
        "#  f.write(f'nc: 6\\n')\n",
        "#  f.write(f'names: [ 0, 1, 2, 3, 4, 5 ]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wepSgclerjYn"
      },
      "source": [
        "## YOLO 모델 Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQkrrs3kwOzQ"
      },
      "outputs": [],
      "source": [
        "###  10번 미만 epoch는 좋은 성능이 안나옴. 최소 30번 이상 epoch 적용. \n",
        "# !cd /content/yolov5; python train.py --img 640 --batch 16 --epochs 30 --data '/content/drive/MyDrive/DACON/Data/AI competition to diagnose crop diseases due to changes in the agricultural environment/crop.yaml' --weights yolov5x.pt --project='/content/drive/MyDrive/DACON/Data/AI competition to diagnose crop diseases due to changes in the agricultural environment/ultra_workdir/Detectionmodel1' \\\n",
        "#                                      --exist-ok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss0km-S4fc_9"
      },
      "source": [
        "## Yolo Detect 및 Cut"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Yolo library"
      ],
      "metadata": {
        "id": "VV-EiWgYHKPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynsOHji0_Vqo",
        "outputId": "02dca436-ef97-4a5f-ac3b-c531f311ca5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzGiQDiT_Vqq"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import timm\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda')\n",
        "def accuracy_function(real, pred):    \n",
        "    score = f1_score(real, pred, average='macro')\n",
        "    return score\n",
        "\n",
        "def model_save(model, score,  path):\n",
        "    os.makedirs('model', exist_ok=True)\n",
        "    torch.save({\n",
        "        'model': model.state_dict(),\n",
        "        'score': score\n",
        "    }, path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crops = list(range(len(train_index)))\n",
        "diseases = list(range(len(train_index)))\n",
        "risks = list(range(len(train_index)))\n",
        "labels = list(range(len(train_index)))\n",
        "bboxs = list(range(len(train_index)))\n",
        "for i in tqdm(range(len(train_index))):\n",
        "    with open(train_index[i]+'.json', 'r') as f:\n",
        "        sample = json.load(f)\n",
        "    crop = sample['annotations']['crop']\n",
        "    disease = sample['annotations']['disease']\n",
        "    risk = sample['annotations']['risk']\n",
        "    label=f\"{crop}_{disease}_{risk}\"\n",
        "    bbox = sample['annotations']['bbox']\n",
        "    crops[i] = crop\n",
        "    diseases[i] = disease\n",
        "    risks[i] = risk\n",
        "    labels[i] = label\n",
        "    bboxs[i] = bbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FduJ0AUp3q_4",
        "outputId": "3a067f6b-68e8-4f58-e46c-7ae6f9bc288c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5767/5767 [00:05<00:00, 1125.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG4yXUWY_Vqq"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "\n",
        "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
        "\n",
        "def test_cut(source, weights, data, \n",
        "        imgsz=(640, 640),  # inference size (height, width)\n",
        "        conf_thres=0.25,  # confidence threshold\n",
        "        iou_thres=0.45,  # NMS IOU threshold\n",
        "        max_det=1,  # maximum detections per image\n",
        "        augment=False,  # augmented inference\n",
        "        exist_ok=False,  # existing project/name ok, do not increment\n",
        "        half=False,  # use FP16 half-precision inference\n",
        "        dnn=False # use OpenCV DNN for ONNX inference\n",
        "        ):  \n",
        "    imgs = []\n",
        "    # Load model\n",
        "\n",
        "    model = DetectMultiBackend(weights, device = device, dnn = dnn, data = data)\n",
        "    stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
        "    \n",
        "    # Half\n",
        "    half &= (pt or jit or onnx or engine) and device.type != 'cpu'  # FP16 supported on limited backends with CUDA\n",
        "    if pt or jit:\n",
        "        model.model.half() if half else model.model.float()\n",
        "\n",
        "    bs = 1  # batch_size\n",
        "\n",
        "    # Run inference\n",
        "    model.warmup(imgsz=(1, 3, *imgsz), half=half)  # warmup\n",
        "    dt, seen = [0.0, 0.0, 0.0], 0\n",
        "    for i in tqdm(range(len(source))):\n",
        "        im = np.ascontiguousarray(source[i].transpose((2,0,1))[::-1])\n",
        "        im0s = source[i]\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if half else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        pred = model(im, augment=augment, visualize=False)\n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2\n",
        "\n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, None, False, max_det=max_det)\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # Second-stage classifier (optional)\n",
        "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            im0 = im0s.copy()\n",
        "\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "        if det.size()[0] > 0:\n",
        "          temp_img = im0s[int(det[0][1]):int(det[0][3]), int(det[0][0]):int(det[0][2])]\n",
        "        else:\n",
        "          temp_img = im0s\n",
        "\n",
        "        temp_img = cv2.resize(temp_img, (384, 512))\n",
        "        \n",
        "        imgs.append(temp_img)\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxhoXGs_96C"
      },
      "source": [
        "### Train Cut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyANcUZS_Vqs"
      },
      "outputs": [],
      "source": [
        "# Ray Task    \n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "@ray.remote\n",
        "def train_img_load(path,bbox):\n",
        "    img = cv2.imread(path)[:,:,::-1]\n",
        "    img = img[int(bbox[0]['y']):(int(bbox[0]['y'])+int(bbox[0]['h'])), int(bbox[0]['x']):(int(bbox[0]['x'])+int(bbox[0]['w']))]\n",
        "    img = cv2.resize(img, (384, 512))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTWUb_Mw_Vqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70960f8-602d-484c-eaa1-0da364c0a6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5767it [00:10, 535.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(train_img_load pid=24920)\u001b[0m \n",
            "소요 시간 :  40.85077214241028\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "result = []\n",
        "for k,pa in tqdm(enumerate(train_index)):\n",
        "  result.append(train_img_load.remote(ray.put(pa+\".jpg\"), ray.put(bboxs[k])))\n",
        "train_img = ray.get(result)\n",
        "ray.shutdown()\n",
        "\n",
        "end = time.time()\n",
        "print('소요 시간 : ', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03UbnfIg__ja"
      },
      "source": [
        "### Test Cut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KEyLRxH_Vqs"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "@ray.remote\n",
        "def test_img_load(path):\n",
        "    img = cv2.imread(path)[:,:,::-1]\n",
        "    img = cv2.resize(img, (384, 512))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_test = []"
      ],
      "metadata": {
        "id": "QDy4Hq0KORvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsFNikiZ_Vqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c536b7ab-6db3-4346-cbb2-75c4c5cdcbdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-17 01:28:53,386\tWARNING services.py:1826 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 6541639680 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=7.76gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
            "100%|██████████| 6906/6906 [00:07<00:00, 975.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(test_img_load pid=28134)\u001b[0m \n",
            "9  소요 시간 :  63.91328740119934\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  start = time.time()\n",
        "\n",
        "  ray.shutdown()\n",
        "  ray.init()\n",
        "  result = []\n",
        "  if i < 9:\n",
        "    for pa in tqdm(test_index[i*5000:(i+1)*5000]):\n",
        "      result.append(test_img_load.remote(ray.put(pa+\".jpg\")))\n",
        "  else:\n",
        "    for pa in tqdm(test_index[i*5000:]):\n",
        "      result.append(test_img_load.remote(ray.put(pa+\".jpg\")))\n",
        "  temp_test.append(ray.get(result)) \n",
        "  end = time.time()\n",
        "  print(i,' 소요 시간 : ', end - start)\n",
        "\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(temp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktn6QTa2qhR1",
        "outputId": "868c09bc-add0-45e1-b2c8-82fec1512f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E50-vehFftyR"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNkv2_X4S0DO"
      },
      "outputs": [],
      "source": [
        "class Custom_dataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, mode='train'):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode=mode\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.img_paths[idx]\n",
        "        \n",
        "        if self.mode=='train':\n",
        "            augmentation = random.randint(0,2)\n",
        "            if augmentation==1:\n",
        "                img = img[::-1].copy()\n",
        "            elif augmentation==2:\n",
        "                img = img[:,::-1].copy()\n",
        "        img = transforms.ToTensor()(img)\n",
        "        if self.mode=='test':\n",
        "            return img\n",
        "        \n",
        "        label = self.labels[idx]\n",
        "        return img,label\n",
        "    \n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=25)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwXwnuJnS9op"
      },
      "outputs": [],
      "source": [
        "folds = []\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for train_idx, valid_idx in kf.split(train_img):\n",
        "    folds.append((train_idx, valid_idx))\n",
        "fold=0\n",
        "train_idx, valid_idx = folds[fold]\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 30\n",
        "\n",
        "\n",
        "train_dataset = Custom_dataset(np.array(train_img)[train_idx], np.array(labels)[train_idx], mode='train')\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, pin_memory=True, num_workers=8)\n",
        "\n",
        "        \n",
        "valid_dataset = Custom_dataset(np.array(train_img)[valid_idx], np.array(labels)[valid_idx], mode='valid')\n",
        "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=batch_size, pin_memory=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aAHmur7S_hI"
      },
      "outputs": [],
      "source": [
        "model = Network().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "\n",
        "best=0\n",
        "for epoch in range(epochs):\n",
        "    start=time.time()\n",
        "    train_loss = 0\n",
        "    train_pred=[]\n",
        "    train_y=[]\n",
        "    model.train()\n",
        "    for batch in (train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item()/len(train_loader)\n",
        "        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "        train_y += y.detach().cpu().numpy().tolist()\n",
        "        \n",
        "    \n",
        "    train_f1 = accuracy_function(train_y, train_pred)\n",
        "    \n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_pred=[]\n",
        "    valid_y=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in (valid_loader):\n",
        "            x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "            y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            valid_loss += loss.item()/len(valid_loader)\n",
        "            valid_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "            valid_y += y.detach().cpu().numpy().tolist()\n",
        "        valid_f1 = accuracy_function(valid_y, valid_pred)\n",
        "    if valid_f1>=best:\n",
        "        best=valid_f1\n",
        "        model_save(model, valid_f1, path + f'/CNN_model/eff-b0.pth')\n",
        "    TIME = time.time() - start\n",
        "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
        "    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n",
        "    print(f'VALID    loss : {valid_loss:.5f}    f1 : {valid_f1:.5f}    best : {best:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = Network()\n",
        "loaded_model.cuda()\n",
        "loaded_model.load_state_dict(torch.load(path + '/CNN_model/eff-b0.pth')['model'])\n",
        "loaded_model.eval"
      ],
      "metadata": {
        "id": "eGl3Ejm6HK2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(temp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjGTKqnmU_aN",
        "outputId": "6b42431e-4aff-48b2-a17b-d352f0ccb45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "test_pred = []\n",
        "\n",
        "for i in range(len(temp_test)):\n",
        "  print(i + 1 , \"번째 Test 리스트\")\n",
        "  test_img = test_cut(source = temp_test[i], weights = path + '/ultra_workdir/Detectionmodel1/exp/weights/best.pt', data = path + '/crop.yaml')\n",
        "\n",
        "\n",
        "  #test_Dataset\n",
        "  temp_label = []\n",
        "  for i in range(len(test_img)):\n",
        "    temp_label.append('5')\n",
        "\n",
        "  test_dataset = Custom_dataset(np.array(test_img),np.array(temp_label), mode='valid')\n",
        "  test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, pin_memory=True, num_workers=8)\n",
        "\n",
        "  #test pred\n",
        "  with torch.no_grad():\n",
        "      for batch in (test_loader):\n",
        "          x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "          with torch.cuda.amp.autocast():\n",
        "              pred = loaded_model(x)\n",
        "          test_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "\n",
        "end = time.time()  \n",
        "print('소요 시간 : ', end - start)"
      ],
      "metadata": {
        "id": "_0dlIghzGt0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7701a2b-eaec-419f-9915-ef8ad0dad76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [02:15<00:00, 37.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [02:20<00:00, 35.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [03:07<00:00, 26.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [03:07<00:00, 26.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [02:51<00:00, 29.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [02:36<00:00, 31.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [02:55<00:00, 28.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [02:40<00:00, 31.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [02:46<00:00, 30.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 번째 Test 리스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6906/6906 [03:22<00:00, 34.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소요 시간 :  2237.8540785312653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_description = [\n",
        "                      \"1_00_0\",\n",
        "                      \"2_00_0\",\n",
        "                      \"2_a5_2\",\n",
        "                      \"3_00_0\",\n",
        "                      \"3_a9_1\",\n",
        "                      \"3_a9_2\",\n",
        "                      \"3_a9_3\",\n",
        "                      \"3_b3_1\",\n",
        "                      \"3_b6_1\",\n",
        "                      \"3_b7_1\",\n",
        "                      \"3_b8_1\",\n",
        "                      \"4_00_0\",\n",
        "                      \"5_00_0\",\n",
        "                      \"5_a7_2\",\n",
        "                      \"5_b6_1\",\n",
        "                      \"5_b7_1\",\n",
        "                      \"5_b8_1\",\n",
        "                      \"6_00_0\",\n",
        "                      \"6_a11_1\",\n",
        "                      \"6_a11_2\",\n",
        "                      \"6_a12_1\",\n",
        "                      \"6_a12_2\",\n",
        "                      \"6_b4_1\",\n",
        "                      \"6_b4_3\",\n",
        "                      \"6_b5_1\"\n",
        "                      ]"
      ],
      "metadata": {
        "id": "snC-nNN5dofl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDjbS0Jg1IGv",
        "outputId": "1eff8503-f0ef-4d00-c6ca-c33a4b5a1263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51906"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_pred = []\n",
        "for i in range(len(test_pred)):\n",
        "  final_pred.append(label_description[test_pred[i]])"
      ],
      "metadata": {
        "id": "QbF8h8ZleINQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(path + '/sample_submission.csv')\n",
        "submission['label'] = final_pred\n",
        "submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qLprZGlPe7OQ",
        "outputId": "6014cb15-9530-4f4b-b8a0-d320f61aa97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-24b61699-da1c-4994-b32a-13d6cd6eb47c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "      <td>6_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001</td>\n",
              "      <td>5_b6_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10002</td>\n",
              "      <td>4_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10003</td>\n",
              "      <td>3_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10004</td>\n",
              "      <td>3_b8_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51901</th>\n",
              "      <td>67673</td>\n",
              "      <td>4_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51902</th>\n",
              "      <td>67674</td>\n",
              "      <td>3_b7_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51903</th>\n",
              "      <td>67675</td>\n",
              "      <td>6_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51904</th>\n",
              "      <td>67676</td>\n",
              "      <td>2_a5_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51905</th>\n",
              "      <td>67677</td>\n",
              "      <td>6_00_0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51906 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24b61699-da1c-4994-b32a-13d6cd6eb47c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24b61699-da1c-4994-b32a-13d6cd6eb47c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24b61699-da1c-4994-b32a-13d6cd6eb47c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       image   label\n",
              "0      10000  6_00_0\n",
              "1      10001  5_b6_1\n",
              "2      10002  4_00_0\n",
              "3      10003  3_00_0\n",
              "4      10004  3_b8_1\n",
              "...      ...     ...\n",
              "51901  67673  4_00_0\n",
              "51902  67674  3_b7_1\n",
              "51903  67675  6_00_0\n",
              "51904  67676  2_a5_2\n",
              "51905  67677  6_00_0\n",
              "\n",
              "[51906 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(path + '/submission/submission.csv', index=False)"
      ],
      "metadata": {
        "id": "vk0uBpFOe_m5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VV-EiWgYHKPX",
        "03UbnfIg__ja",
        "E50-vehFftyR"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}