{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoGyeongDeok/Project/blob/main/Dacon/2022_1_17_AI_competition_to_diagnose_crop_diseases_due_to_changes_in_the_agricultural_environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI competition to diagnose crop diseases due to changes in the agricultural environment\n",
        "***https://www.dacon.io/competitions/official/235870/overview/description***"
      ],
      "metadata": {
        "id": "RYrcxEeAT5eL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8znKRTOtklN_"
      },
      "source": [
        "# 1. Library & Data Load & í•¨ìˆ˜ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgsd5wE06GHT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuLMM1L_FN2G"
      },
      "outputs": [],
      "source": [
        "!pip install ray                                    #ë³‘ë ¬ì²˜ë¦¬ \n",
        "!git clone https://github.com/ultralytics/yolov5    #Yolo\n",
        "!cd yolov5; pip install -qr requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgKKGur649nE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5UJIazHya1r"
      },
      "outputs": [],
      "source": [
        "cp -r '/content/yolov5/utils' '/content'\n",
        "cp -r '/content/yolov5/models' '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ico0McsFs1Fy"
      },
      "outputs": [],
      "source": [
        "path= '/content/drive/MyDrive/DACON/Data/AI competition to diagnose crop diseases due to changes in the agricultural environment'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iRpcN8pzyFF"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import os\n",
        "import json \n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import yaml\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import ray\n",
        "import datetime\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-E_Gk2dFATH"
      },
      "outputs": [],
      "source": [
        "#os.mkdir(path)\n",
        "# # zipfile.ZipFile(path+'.zip','r').extractall(path+'/')\n",
        "# # zipfile.ZipFile(path+'/train.zip','r').extractall(path+'/')\n",
        "# # zipfile.ZipFile(path+'/test.zip','r').extractall(path+'/')\n",
        "\n",
        "#os.mkdir(path+'/images')\n",
        "#os.mkdir(path+'/labels')\n",
        "\n",
        "#os.mkdir(path+'/images/train')\n",
        "#os.mkdir(path+'/images/valid')\n",
        "#os.mkdir(path+'/images/test')\n",
        "\n",
        "#os.mkdir(path+'/labels/train')\n",
        "#os.mkdir(path+'/labels/valid')\n",
        "#os.mkdir(path+'/labels/test')\n",
        "\n",
        "# os.mkdir(path + '/ultra_workdir')\n",
        "# os.makedirs(path + '/CNN_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6LRSMlJlGq6"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print('Using PyTorch version : ', torch.__version__, 'Device : ', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZUpO4tladu_"
      },
      "source": [
        "# 2. Object Deteion ìˆ˜í–‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMCbdX7rAy4"
      },
      "source": [
        "## Yolo í˜•ì‹ ë§žì¶°ì„œ ì´ë¯¸ì§€, annotation íŒŒì¼ ì €ìž¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwGKzbDgWRGv"
      },
      "outputs": [],
      "source": [
        "test_index = []\n",
        "for _ in range(10):\n",
        "  if len(test_index) != 0 :\n",
        "    break\n",
        "\n",
        "  train_index = sorted(glob(path+\"/train/*\"))\n",
        "  test_index = sorted(glob(path+\"/test/*\"))\n",
        "  \n",
        "  for num in range(len(train_index)):\n",
        "    train_index[num] = train_index[num] + train_index[num][len(path + '/train'):]\n",
        "    \n",
        "  for num in range(len(test_index)):\n",
        "    test_index[num] = test_index[num] + test_index[num][len(path + '/test'):]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3q4tfwymmid",
        "outputId": "9516b43e-15bc-4b8a-cad5-822ee206116f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51906"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#51906\n",
        "len(test_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM7zmcG7HyPa",
        "outputId": "c86f477e-883d-48de-f739-c3e5f0874f27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5767"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#5767\n",
        "len(train_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLOTpxoXAq47"
      },
      "outputs": [],
      "source": [
        "# 1ê°œì˜ voc xml íŒŒì¼ì„ Yolo í¬ë§·ìš© txt íŒŒì¼ë¡œ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜ \n",
        "def json_to_txt(input_xml_file, output_txt_file):\n",
        "  # ì›ë³¸ ì´ë¯¸ì§€ì˜ ë„ˆë¹„ì™€ ë†’ì´ ì¶”ì¶œ. \n",
        "  with open(input_xml_file, 'r', encoding='UTF-8') as json_file:\n",
        "    json_file = json.load(json_file)\n",
        "    img_width = int(json_file['description']['width'])\n",
        "    img_height = int(json_file['description']['height'])\n",
        "    x1 = int(json_file['annotations']['bbox'][0]['x'])\n",
        "    y1 = int(json_file['annotations']['bbox'][0]['y'])\n",
        "    x2 = int(json_file['annotations']['bbox'][0]['x']) + int(json_file['annotations']['bbox'][0]['w'])\n",
        "    y2 = int(json_file['annotations']['bbox'][0]['y']) + int(json_file['annotations']['bbox'][0]['h'])\n",
        "    object_name = json_file['annotations']['crop']\n",
        "\n",
        "  # object_nameê³¼ ì›ë³¸ ì¢Œí‘œë¥¼ ìž…ë ¥í•˜ì—¬ Yolo í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” convert_yolo_coord()í•¨ìˆ˜ í˜¸ì¶œ. \n",
        "  cx_norm, cy_norm, w_norm, h_norm = convert_yolo_coord(object_name, img_width, img_height, x1, y1, x2, y2)\n",
        "  # ë³€í™˜ëœ yolo ì¢Œí‘œë¥¼ object ë³„ë¡œ ì¶œë ¥ text íŒŒì¼ì— write\n",
        "  value_str = ('{0} {1} {2} {3} {4}').format(object_name-1, cx_norm, cy_norm, w_norm, h_norm)\n",
        "  with open(output_txt_file, 'w') as output_fpointer:\n",
        "    output_fpointer.write(value_str+'\\n')\n",
        "\n",
        "\n",
        "# object_nameê³¼ ì›ë³¸ ì¢Œí‘œë¥¼ ìž…ë ¥í•˜ì—¬ Yolo í¬ë§·ìœ¼ë¡œ ë³€í™˜\n",
        "def convert_yolo_coord(object_name, img_width, img_height, x1, y1, x2, y2):\n",
        "  # ì¤‘ì‹¬ ì¢Œí‘œì™€ ë„ˆë¹„, ë†’ì´ ê³„ì‚°. \n",
        "  center_x = (x1 + x2)/2\n",
        "  center_y = (y1 + y2)/2\n",
        "  width = x2 - x1\n",
        "  height = y2 - y1\n",
        "  # ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ì‹¬ ì¢Œí‘œì™€ ë„ˆë¹„ ë†’ì´ë¥¼ 0-1 ì‚¬ì´ ê°’ìœ¼ë¡œ scaling\n",
        "  center_x_norm = center_x / img_width\n",
        "  center_y_norm = center_y / img_height\n",
        "  width_norm = width / img_width\n",
        "  height_norm = height / img_height\n",
        "\n",
        "  return round(center_x_norm, 7), round(center_y_norm, 7), round(width_norm, 7), round(height_norm, 7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.mov"
      ],
      "metadata": {
        "id": "k6k_BnC4gkbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS3-QTAQnzEJ"
      },
      "outputs": [],
      "source": [
        "def make_yolo_anno_file(index, tgt_images_dir, tgt_labels_dir, image, label):\n",
        "  for row in tqdm(index):\n",
        "    src_image_path = row + '.jpg'\n",
        "    src_label_path = row + '.json'\n",
        "    # yolo formatìœ¼ë¡œ annotationí•  txt íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œëª…ì„ ì§€ì •. \n",
        "      \n",
        "    if image == True : \n",
        "      # imageì˜ ê²½ìš° target images ë””ë ‰í† ë¦¬ë¡œ ë‹¨ìˆœ copy\n",
        "      shutil.copy(src_image_path, tgt_images_dir)     \n",
        "\n",
        "    if label == True : \n",
        "      target_label_path = tgt_labels_dir + row[-5:]+'.txt'\n",
        "      # annotationì˜ ê²½ìš° json íŒŒì¼ì„ target labels ë””ë ‰í† ë¦¬ì— Ultralytics Yolo formatìœ¼ë¡œ ë³€í™˜í•˜ì—¬  ë§Œë“¬\n",
        "      json_to_txt(src_label_path, target_label_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9W1e__hBZHL"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "valid_index = random.sample(train_index,int(len(train_index)*0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcezixSrmcFd"
      },
      "outputs": [],
      "source": [
        "# # # trainìš© imagesì™€ labels annotation ìƒì„±. \n",
        "# make_yolo_anno_file(train_index, path+'/images/train/', path+'/labels/train/', image = True, label = True)\n",
        "make_yolo_anno_file(valid_index, path+'/images/valid/', path+'/labels/valid/', image = True, label = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38FZ0NNyFpFL"
      },
      "outputs": [],
      "source": [
        "# yamlíŒŒì¼ ìž‘ì„±\n",
        "#with open(path+\"/crop.yaml\",\"w\") as f:\n",
        "#  f.write(f'train: {path}/images/train/\\n')\n",
        "#  f.write(f'val: {path}/images/valid/\\n')  \n",
        "#  f.write(f'test: {path}/images/test/\\n')\n",
        "#  f.write(f'nc: 6\\n')\n",
        "#  f.write(f'names: [ 0, 1, 2, 3, 4, 5 ]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wepSgclerjYn"
      },
      "source": [
        "## YOLO ëª¨ë¸ Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQkrrs3kwOzQ"
      },
      "outputs": [],
      "source": [
        "###  10ë²ˆ ë¯¸ë§Œ epochëŠ” ì¢‹ì€ ì„±ëŠ¥ì´ ì•ˆë‚˜ì˜´. ìµœì†Œ 30ë²ˆ ì´ìƒ epoch ì ìš©. \n",
        "# !cd /content/yolov5; python train.py --img 640 --batch 16 --epochs 30 --data '/content/drive/MyDrive/DACON/Data/AI competition to diagnose crop diseases due to changes in the agricultural environment/crop.yaml' --weights yolov5x.pt --project='/content/drive/MyDrive/DACON/Data/AI competition to diagnose crop diseases due to changes in the agricultural environment/ultra_workdir/Detectionmodel1' \\\n",
        "#                                      --exist-ok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss0km-S4fc_9"
      },
      "source": [
        "## Yolo Detect ë° Cut"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Yolo library"
      ],
      "metadata": {
        "id": "VV-EiWgYHKPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynsOHji0_Vqo",
        "outputId": "02dca436-ef97-4a5f-ac3b-c531f311ca5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzGiQDiT_Vqq"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import timm\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda')\n",
        "def accuracy_function(real, pred):    \n",
        "    score = f1_score(real, pred, average='macro')\n",
        "    return score\n",
        "\n",
        "def model_save(model, score,  path):\n",
        "    os.makedirs('model', exist_ok=True)\n",
        "    torch.save({\n",
        "        'model': model.state_dict(),\n",
        "        'score': score\n",
        "    }, path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crops = list(range(len(train_index)))\n",
        "diseases = list(range(len(train_index)))\n",
        "risks = list(range(len(train_index)))\n",
        "labels = list(range(len(train_index)))\n",
        "bboxs = list(range(len(train_index)))\n",
        "for i in tqdm(range(len(train_index))):\n",
        "    with open(train_index[i]+'.json', 'r') as f:\n",
        "        sample = json.load(f)\n",
        "    crop = sample['annotations']['crop']\n",
        "    disease = sample['annotations']['disease']\n",
        "    risk = sample['annotations']['risk']\n",
        "    label=f\"{crop}_{disease}_{risk}\"\n",
        "    bbox = sample['annotations']['bbox']\n",
        "    crops[i] = crop\n",
        "    diseases[i] = disease\n",
        "    risks[i] = risk\n",
        "    labels[i] = label\n",
        "    bboxs[i] = bbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FduJ0AUp3q_4",
        "outputId": "3a067f6b-68e8-4f58-e46c-7ae6f9bc288c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5767/5767 [00:05<00:00, 1125.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG4yXUWY_Vqq"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "\n",
        "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
        "\n",
        "def test_cut(source, weights, data, \n",
        "        imgsz=(640, 640),  # inference size (height, width)\n",
        "        conf_thres=0.25,  # confidence threshold\n",
        "        iou_thres=0.45,  # NMS IOU threshold\n",
        "        max_det=1,  # maximum detections per image\n",
        "        augment=False,  # augmented inference\n",
        "        exist_ok=False,  # existing project/name ok, do not increment\n",
        "        half=False,  # use FP16 half-precision inference\n",
        "        dnn=False # use OpenCV DNN for ONNX inference\n",
        "        ):  \n",
        "    imgs = []\n",
        "    # Load model\n",
        "\n",
        "    model = DetectMultiBackend(weights, device = device, dnn = dnn, data = data)\n",
        "    stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
        "    \n",
        "    # Half\n",
        "    half &= (pt or jit or onnx or engine) and device.type != 'cpu'  # FP16 supported on limited backends with CUDA\n",
        "    if pt or jit:\n",
        "        model.model.half() if half else model.model.float()\n",
        "\n",
        "    bs = 1  # batch_size\n",
        "\n",
        "    # Run inference\n",
        "    model.warmup(imgsz=(1, 3, *imgsz), half=half)  # warmup\n",
        "    dt, seen = [0.0, 0.0, 0.0], 0\n",
        "    for i in tqdm(range(len(source))):\n",
        "        im = np.ascontiguousarray(source[i].transpose((2,0,1))[::-1])\n",
        "        im0s = source[i]\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if half else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        pred = model(im, augment=augment, visualize=False)\n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2\n",
        "\n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, None, False, max_det=max_det)\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # Second-stage classifier (optional)\n",
        "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            im0 = im0s.copy()\n",
        "\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "        if det.size()[0] > 0:\n",
        "          temp_img = im0s[int(det[0][1]):int(det[0][3]), int(det[0][0]):int(det[0][2])]\n",
        "        else:\n",
        "          temp_img = im0s\n",
        "\n",
        "        temp_img = cv2.resize(temp_img, (384, 512))\n",
        "        \n",
        "        imgs.append(temp_img)\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxhoXGs_96C"
      },
      "source": [
        "### Train Cut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyANcUZS_Vqs"
      },
      "outputs": [],
      "source": [
        "# Ray Task    \n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "@ray.remote\n",
        "def train_img_load(path,bbox):\n",
        "    img = cv2.imread(path)[:,:,::-1]\n",
        "    img = img[int(bbox[0]['y']):(int(bbox[0]['y'])+int(bbox[0]['h'])), int(bbox[0]['x']):(int(bbox[0]['x'])+int(bbox[0]['w']))]\n",
        "    img = cv2.resize(img, (384, 512))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTWUb_Mw_Vqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70960f8-602d-484c-eaa1-0da364c0a6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5767it [00:10, 535.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(train_img_load pid=24920)\u001b[0m \n",
            "ì†Œìš” ì‹œê°„ :  40.85077214241028\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "result = []\n",
        "for k,pa in tqdm(enumerate(train_index)):\n",
        "  result.append(train_img_load.remote(ray.put(pa+\".jpg\"), ray.put(bboxs[k])))\n",
        "train_img = ray.get(result)\n",
        "ray.shutdown()\n",
        "\n",
        "end = time.time()\n",
        "print('ì†Œìš” ì‹œê°„ : ', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03UbnfIg__ja"
      },
      "source": [
        "### Test Cut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KEyLRxH_Vqs"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "@ray.remote\n",
        "def test_img_load(path):\n",
        "    img = cv2.imread(path)[:,:,::-1]\n",
        "    img = cv2.resize(img, (384, 512))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_test = []"
      ],
      "metadata": {
        "id": "QDy4Hq0KORvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsFNikiZ_Vqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c536b7ab-6db3-4346-cbb2-75c4c5cdcbdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-17 01:28:53,386\tWARNING services.py:1826 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 6541639680 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=7.76gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6906/6906 [00:07<00:00, 975.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(test_img_load pid=28134)\u001b[0m \n",
            "9  ì†Œìš” ì‹œê°„ :  63.91328740119934\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  start = time.time()\n",
        "\n",
        "  ray.shutdown()\n",
        "  ray.init()\n",
        "  result = []\n",
        "  if i < 9:\n",
        "    for pa in tqdm(test_index[i*5000:(i+1)*5000]):\n",
        "      result.append(test_img_load.remote(ray.put(pa+\".jpg\")))\n",
        "  else:\n",
        "    for pa in tqdm(test_index[i*5000:]):\n",
        "      result.append(test_img_load.remote(ray.put(pa+\".jpg\")))\n",
        "  temp_test.append(ray.get(result)) \n",
        "  end = time.time()\n",
        "  print(i,' ì†Œìš” ì‹œê°„ : ', end - start)\n",
        "\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(temp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktn6QTa2qhR1",
        "outputId": "868c09bc-add0-45e1-b2c8-82fec1512f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E50-vehFftyR"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNkv2_X4S0DO"
      },
      "outputs": [],
      "source": [
        "class Custom_dataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, mode='train'):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode=mode\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.img_paths[idx]\n",
        "        \n",
        "        if self.mode=='train':\n",
        "            augmentation = random.randint(0,2)\n",
        "            if augmentation==1:\n",
        "                img = img[::-1].copy()\n",
        "            elif augmentation==2:\n",
        "                img = img[:,::-1].copy()\n",
        "        img = transforms.ToTensor()(img)\n",
        "        if self.mode=='test':\n",
        "            return img\n",
        "        \n",
        "        label = self.labels[idx]\n",
        "        return img,label\n",
        "    \n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=25)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwXwnuJnS9op"
      },
      "outputs": [],
      "source": [
        "folds = []\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for train_idx, valid_idx in kf.split(train_img):\n",
        "    folds.append((train_idx, valid_idx))\n",
        "fold=0\n",
        "train_idx, valid_idx = folds[fold]\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 30\n",
        "\n",
        "\n",
        "train_dataset = Custom_dataset(np.array(train_img)[train_idx], np.array(labels)[train_idx], mode='train')\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, pin_memory=True, num_workers=8)\n",
        "\n",
        "        \n",
        "valid_dataset = Custom_dataset(np.array(train_img)[valid_idx], np.array(labels)[valid_idx], mode='valid')\n",
        "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=batch_size, pin_memory=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aAHmur7S_hI"
      },
      "outputs": [],
      "source": [
        "model = Network().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "\n",
        "best=0\n",
        "for epoch in range(epochs):\n",
        "    start=time.time()\n",
        "    train_loss = 0\n",
        "    train_pred=[]\n",
        "    train_y=[]\n",
        "    model.train()\n",
        "    for batch in (train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item()/len(train_loader)\n",
        "        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "        train_y += y.detach().cpu().numpy().tolist()\n",
        "        \n",
        "    \n",
        "    train_f1 = accuracy_function(train_y, train_pred)\n",
        "    \n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_pred=[]\n",
        "    valid_y=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in (valid_loader):\n",
        "            x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "            y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            valid_loss += loss.item()/len(valid_loader)\n",
        "            valid_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "            valid_y += y.detach().cpu().numpy().tolist()\n",
        "        valid_f1 = accuracy_function(valid_y, valid_pred)\n",
        "    if valid_f1>=best:\n",
        "        best=valid_f1\n",
        "        model_save(model, valid_f1, path + f'/CNN_model/eff-b0.pth')\n",
        "    TIME = time.time() - start\n",
        "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
        "    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n",
        "    print(f'VALID    loss : {valid_loss:.5f}    f1 : {valid_f1:.5f}    best : {best:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = Network()\n",
        "loaded_model.cuda()\n",
        "loaded_model.load_state_dict(torch.load(path + '/CNN_model/eff-b0.pth')['model'])\n",
        "loaded_model.eval"
      ],
      "metadata": {
        "id": "eGl3Ejm6HK2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(temp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjGTKqnmU_aN",
        "outputId": "6b42431e-4aff-48b2-a17b-d352f0ccb45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "test_pred = []\n",
        "\n",
        "for i in range(len(temp_test)):\n",
        "  print(i + 1 , \"ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\")\n",
        "  test_img = test_cut(source = temp_test[i], weights = path + '/ultra_workdir/Detectionmodel1/exp/weights/best.pt', data = path + '/crop.yaml')\n",
        "\n",
        "\n",
        "  #test_Dataset\n",
        "  temp_label = []\n",
        "  for i in range(len(test_img)):\n",
        "    temp_label.append('5')\n",
        "\n",
        "  test_dataset = Custom_dataset(np.array(test_img),np.array(temp_label), mode='valid')\n",
        "  test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, pin_memory=True, num_workers=8)\n",
        "\n",
        "  #test pred\n",
        "  with torch.no_grad():\n",
        "      for batch in (test_loader):\n",
        "          x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "          with torch.cuda.amp.autocast():\n",
        "              pred = loaded_model(x)\n",
        "          test_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "\n",
        "end = time.time()  \n",
        "print('ì†Œìš” ì‹œê°„ : ', end - start)"
      ],
      "metadata": {
        "id": "_0dlIghzGt0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7701a2b-eaec-419f-9915-ef8ad0dad76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:15<00:00, 37.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:20<00:00, 35.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [03:07<00:00, 26.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [03:07<00:00, 26.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:51<00:00, 29.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:36<00:00, 31.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:55<00:00, 28.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:40<00:00, 31.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:46<00:00, 30.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 ë²ˆì§¸ Test ë¦¬ìŠ¤íŠ¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 444 layers, 86207059 parameters, 0 gradients, 204.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6906/6906 [03:22<00:00, 34.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì†Œìš” ì‹œê°„ :  2237.8540785312653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_description = [\n",
        "                      \"1_00_0\",\n",
        "                      \"2_00_0\",\n",
        "                      \"2_a5_2\",\n",
        "                      \"3_00_0\",\n",
        "                      \"3_a9_1\",\n",
        "                      \"3_a9_2\",\n",
        "                      \"3_a9_3\",\n",
        "                      \"3_b3_1\",\n",
        "                      \"3_b6_1\",\n",
        "                      \"3_b7_1\",\n",
        "                      \"3_b8_1\",\n",
        "                      \"4_00_0\",\n",
        "                      \"5_00_0\",\n",
        "                      \"5_a7_2\",\n",
        "                      \"5_b6_1\",\n",
        "                      \"5_b7_1\",\n",
        "                      \"5_b8_1\",\n",
        "                      \"6_00_0\",\n",
        "                      \"6_a11_1\",\n",
        "                      \"6_a11_2\",\n",
        "                      \"6_a12_1\",\n",
        "                      \"6_a12_2\",\n",
        "                      \"6_b4_1\",\n",
        "                      \"6_b4_3\",\n",
        "                      \"6_b5_1\"\n",
        "                      ]"
      ],
      "metadata": {
        "id": "snC-nNN5dofl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDjbS0Jg1IGv",
        "outputId": "1eff8503-f0ef-4d00-c6ca-c33a4b5a1263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51906"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_pred = []\n",
        "for i in range(len(test_pred)):\n",
        "  final_pred.append(label_description[test_pred[i]])"
      ],
      "metadata": {
        "id": "QbF8h8ZleINQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(path + '/sample_submission.csv')\n",
        "submission['label'] = final_pred\n",
        "submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qLprZGlPe7OQ",
        "outputId": "6014cb15-9530-4f4b-b8a0-d320f61aa97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-24b61699-da1c-4994-b32a-13d6cd6eb47c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "      <td>6_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001</td>\n",
              "      <td>5_b6_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10002</td>\n",
              "      <td>4_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10003</td>\n",
              "      <td>3_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10004</td>\n",
              "      <td>3_b8_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51901</th>\n",
              "      <td>67673</td>\n",
              "      <td>4_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51902</th>\n",
              "      <td>67674</td>\n",
              "      <td>3_b7_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51903</th>\n",
              "      <td>67675</td>\n",
              "      <td>6_00_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51904</th>\n",
              "      <td>67676</td>\n",
              "      <td>2_a5_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51905</th>\n",
              "      <td>67677</td>\n",
              "      <td>6_00_0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51906 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24b61699-da1c-4994-b32a-13d6cd6eb47c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24b61699-da1c-4994-b32a-13d6cd6eb47c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24b61699-da1c-4994-b32a-13d6cd6eb47c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       image   label\n",
              "0      10000  6_00_0\n",
              "1      10001  5_b6_1\n",
              "2      10002  4_00_0\n",
              "3      10003  3_00_0\n",
              "4      10004  3_b8_1\n",
              "...      ...     ...\n",
              "51901  67673  4_00_0\n",
              "51902  67674  3_b7_1\n",
              "51903  67675  6_00_0\n",
              "51904  67676  2_a5_2\n",
              "51905  67677  6_00_0\n",
              "\n",
              "[51906 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(path + '/submission/submission.csv', index=False)"
      ],
      "metadata": {
        "id": "vk0uBpFOe_m5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VV-EiWgYHKPX",
        "03UbnfIg__ja",
        "E50-vehFftyR"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}